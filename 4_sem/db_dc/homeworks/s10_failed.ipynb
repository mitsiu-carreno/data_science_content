import json
import sys
import requests
from pyspark.sql import SparkSession
import os
from pyspark.sql.functions import lit


spark = SparkSession.builder \
    .appName("country_data_analysis") \
    .master("spark://spark-cluster-master-0.spark-cluster-headless.spark-cluster.svc.cluster.local:7077") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026") \
    .config("spark.executorEnv.JUPYTER_HOST", "10.244.0.54") \
    .config("spark.driver.host", "10.244.0.54") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio.minio.svc.cluster.local:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "rootuser") \
    .config("spark.hadoop.fs.s3a.secret.key", "rootpass123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .enableHiveSupport() \
    .getOrCreate()

df = spark.read.text("s3a://demo/input/access.log")

df.show()

df.coalesce(1).write.format("text").option("header", "false").mode("append").save("s3a://demo/output/1.txt")



###______________________________________________________
# Create a dataframe with a single row and column containing the value 1
df = spark.createDataFrame([(1,)], ["value"])

# Add 1 to the value of the dataframe
df_with_sum = df.withColumn("sum", df["value"] + lit(1))

df_with_sum.show()







from pyspark import SparkConf, SparkContext
#conf = (SparkConf().setMaster().set().set())
sc = SparkContext(conf=conf)

# Calculate the approximate sum of values in the dataset
t = sc.parallelize(range(10))
r = t.sumApprox(3)
print('Approximate sum: %s' % r)
