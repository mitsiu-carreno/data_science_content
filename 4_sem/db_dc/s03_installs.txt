#basex/basexhttp:latest

spark:scala

podman run -it --volume ../../3_sem/big_data/homeworks/tweets/data:/data/tweets:Z --volume ./data/:/data/new:Z --expose 4040 -p 4040:4040 spark:latest /opt/spark/bin/spark-shell

user:admin
pass: <empty>
podman run -it -d -p 8080:8080 -p 8443:8443 --name exist existdb/existdb:latest

Upload file to exist-db
curl -s -f -H 'Content-Type: application/xml' -T comtrade_sdmx.xml  --user admin:  "http://localhost:8080/exist/rest/db/apps/data/comtrade_sdmx.xml"

for $result in doc(..)...
return $result

doc("/db/data/mondial.xml")
doc("/db/data/mondial.xml")//country/name
doc("/db/data/mondial.xml")//country[@area<28750]
doc("/db/data/mondial.xml")//country/@area
count(doc("/db/data/mondial.xml")//country/city)
doc("/db/data/mondial.xml")//country/city/population[@measured="census" and @year>2000]

doc("/db/data/mondial.xml")//country/city/population[@measured="estimate" and @year=2020]
vs
for $country in doc("/db/data/mondial.xml")//country
where $country/population/@year=2020 and $country/population/@measured="census"
return <entry><name>{$country/name}</name> <pop>{$country/population}</pop></entry>

for $country in doc("/db/data/mondial.xml")//country
where $country/border/@country="R"
order by $country/name ascending
return $country/name

for $country in doc("/db/data/mondial.xml")//country
where ($country/religion = "Catholic")
return <country name="{$country/name}">
{
    for $religion in $country/religion
    where $religion = "Catholic"
    return data($religion/@percentage)
}
</country>






XML (https://github.com/databricks/spark-xml?tab=readme-ov-file#scala-api)
https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/

podman run -it --volume ${PWD}:/data:Z --expose 4040 -p 4040:4040 -u 0 spark:latest /opt/spark/bin/spark-shell --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" --packages com.databricks:spark-xml_2.12:0.18.0
podman run -it --volume ${PWD}:/data:Z --expose 4040 -p 4040:4040 -u 0 spark:latest /opt/spark/bin/spark-shell --packages com.databricks:spark-xml_2.12:0.18.0

val df = spark.read.format("xml").option("rootTag", "items").option("rowTag", "item").load("/data/best.xml")

#df.select($"*", $"comments.*")
# Select all rows and all subrows in comments, remove comments => comments pass from being an struct into an internal array
val test1 = df.select("*","comments.*").drop("comments")  

# Each element in "value" array is turned into a row
val test2 = test1.withColumn("value", explode(test1("value"))) 

# Take all columns from test2, left join (take all sub cols in value, rename to comment_<column>)  finally drop struct value
ATENCIÃ“N GENERA DUPLICADOS LITERAL ES UN JOIN
val test3 = test2.join(test2.select("value.*").withColumnRenamed("dc_creator", "comment_dc_creator").withColumnRenamed("description", "comment_description").withColumnRenamed("guid", "comment_guid").withColumnRenamed("id", "comment_id").withColumnRenamed("link", "comment_link").withColumnRenamed("pubDate", "comment_pubDate").withColumnRenamed("title", "comment_title")).drop("value")

.show(false) # dont trim

df.write.parquet("/data/test.parquet")
df.coalesce(1).write.parquet("/data/test.parquet")



