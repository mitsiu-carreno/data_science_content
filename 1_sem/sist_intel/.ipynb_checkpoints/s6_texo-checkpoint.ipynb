{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NvNAlkH8vAn"
   },
   "source": [
    "# Extracción de información de texto\n",
    "\n",
    "En este cuaderno utilizaremos librerías de Python para extraer información te texto libre.\n",
    "\n",
    "Estas técnicas son parte de la rama de inteligencia artificial llamada procesamiento de lenguaje natural (NLP por sus siglas en inglés).\n",
    "La librería principal para extraer información de textos será nltk y scikitlearn, pero también utilizaremos librerías request para descargar información, collections para obtener conteos y pandas para crear tablas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P6pL62fWB3VO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JdyLDNG8vWS"
   },
   "source": [
    "# Preparación\n",
    "Descargamosun libro del proyecto [Gutenberg](https://www.gutenberg.org). Este proyecto ofrece libros en distintos idiomas de manera gratuita libres de derechos de autor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x_LCDLiuB7iO"
   },
   "outputs": [],
   "source": [
    "url_libro = \"https://www.gutenberg.org/files/57654/57654-0.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8xZ09oP8vqH"
   },
   "source": [
    "Con este libro generamos un documento donde guardamos el contenido de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF-pXAG3An75"
   },
   "outputs": [],
   "source": [
    "r = requests.get(url_libro)\n",
    "r.encoding = r.apparent_encoding\n",
    "documento = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQaZ3qur8wET"
   },
   "source": [
    "La primera manera de tokenizar los elementos del texto es a través de la función split, la cual separa por el caracter de espacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuTOH2uZDcqm"
   },
   "outputs": [],
   "source": [
    "tokens1 = documento.split(\" \")\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibn8_5z98wYq"
   },
   "source": [
    "Una segunda aproximación es realizar un preprocesamiento eliminando todos  los signos del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbwPk84tEBrQ"
   },
   "outputs": [],
   "source": [
    "separadores = [\"(\",\")\",\",\",\".\",\";\",\":\",\"\\\"\",\"¿\",\"?\",\"¡\",\"!\",\"--\",\"_\",\"\\r\\n\"]\n",
    "documento2 = documento\n",
    "for separador in separadores:\n",
    "  documento2 = documento2.replace(separador,\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWdegWIi8wzg"
   },
   "source": [
    "Una vez que se tiene el corpus libre de signos se procede a separar por palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyFF2fI_El0T"
   },
   "outputs": [],
   "source": [
    "tokens2 = documento2.split(\" \") \n",
    "tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUIeG4628xGY"
   },
   "source": [
    "La tercera opción hace uso de la librería nltk la cual tiene un tokenizador adaptado para cada idioma.\n",
    "\n",
    "Es necesario descargar primero los elementos para el tokenizador ya que no los incluye por defecto la libreria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uWw9MNmPRpC"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6vrB_zxE8D9"
   },
   "outputs": [],
   "source": [
    "tokens3 = nltk.word_tokenize(documento,\"spanish\")\n",
    "tokens3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyI3E7-A8yo1"
   },
   "source": [
    "A continuación definimos funciones para tokenizar y stemizar las palabras a fin de tener conceptos más generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-nqE6wXFqfD"
   },
   "outputs": [],
   "source": [
    "def tokenizar(texto):\n",
    "    puntuacion = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¿¡'\n",
    "    tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "    for i,token in enumerate(tokens):\n",
    "        tokens[i] = token.strip(puntuacion)\n",
    "    texto = \" \".join(tokens)\n",
    "    tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "    return tokens\n",
    "def stemmizar(tokens):\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"spanish\")\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQAJUirZBxnJ"
   },
   "outputs": [],
   "source": [
    "tokens4 = tokenizar (documento)\n",
    "tokens4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_UKrjbZ8y6X"
   },
   "source": [
    "Generamos los tokens y los steams y con estos calculamos un conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfYy_f11FybT"
   },
   "outputs": [],
   "source": [
    "stems = stemmizar(tokens4)\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wxu-woACF5VL"
   },
   "outputs": [],
   "source": [
    "conteos = Counter(stems)\n",
    "conteos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QDIF_Xj8zov"
   },
   "source": [
    "Podemos volcar esta información en un dataframe de pandas para facilitar el manejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5yrKnJ3S0Tw"
   },
   "outputs": [],
   "source": [
    "dataframes = pd.DataFrame.from_dict(conteos, orient='index', columns=[f\"conteo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUyG22jO808I"
   },
   "source": [
    "Encontramos que las palabras más utilizadas son conectores o también llamadas palabras funcionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOMmsmLzgsZx"
   },
   "outputs": [],
   "source": [
    "dataframes.sort_values(\"conteo\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qilTVkzT81hG"
   },
   "source": [
    "La librería nltk proporciona un conjunto de palabras a ser omitidas para el idioma español.\n",
    "\n",
    "Es necesario primero descargar los datos para poder utilizarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C9bbn26hrho"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "palabras_funcionales=nltk.corpus.stopwords.words(\"spanish\")\n",
    "palabras_funcionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaGSibFg82hf"
   },
   "source": [
    "Definimos un tokenizador el cual obtendrá los stems a partir de los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzUDtOCliQlT"
   },
   "outputs": [],
   "source": [
    "def tokenizador(texto):\n",
    "    tokens = tokenizar(texto)\n",
    "    stems = stemmizar(tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UseBRn7u83VT"
   },
   "source": [
    "Los pasos anteriores pueden ser realizados a través de la función CountVectorizer la cual nos permite de manera sencilla establecer tokenizadores y stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxId2gV4jvII"
   },
   "outputs": [],
   "source": [
    "#vectorizador = CountVectorizer(input=\"content\",analyzer=\"word\")\n",
    "#vectorizador = CountVectorizer(input=\"content\",analyzer=\"word\",tokenizer=tokenizador)\n",
    "vectorizador = CountVectorizer(input=\"content\",analyzer=\"word\",stop_words=palabras_funcionales)\n",
    "matriz_frecuencias = vectorizador.fit_transform([documento])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I909lGrh84T2"
   },
   "source": [
    "Volcamos la información a un dataframe, con el cual podemos revisar las frecuencias de los elementos más comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsen_Q2VkIsx"
   },
   "outputs": [],
   "source": [
    "tabla_frecuencias = pd.DataFrame(\n",
    "    np.transpose(matriz_frecuencias.toarray()),\n",
    "    index=vectorizador.get_feature_names_out(), \n",
    "    columns = [\"conteo\"]\n",
    ")\n",
    "tabla_frecuencias.sort_values(\"conteo\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vZEj6Y185aK"
   },
   "source": [
    "También existe el vectorizador tfidf del cual haremos uso para calcular los valores en los documentos presentados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hG3VY6zwks79"
   },
   "outputs": [],
   "source": [
    "vectorizador = TfidfVectorizer(input=\"content\",analyzer=\"word\")\n",
    "matriz_tfidf = vectorizador.fit_transform([documento])\n",
    "tabla_tfidf = pd.DataFrame(np.transpose(matriz_tfidf.toarray()),index=vectorizador.get_feature_names_out(), columns = [\"conteo\"])\n",
    "tabla_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s3X4VMQ86T8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXua4UPHoWQQ"
   },
   "outputs": [],
   "source": [
    "tabla_frecuencias.sort_values(\"conteo\",ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOp8PJX9RQvaSE6vkkky1Aa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
