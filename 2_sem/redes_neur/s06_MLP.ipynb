{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff7320-mMJJA"
   },
   "source": [
    "# Perceptrón multicapa (MLP)\n",
    "\n",
    "Un perceptrón multicapa, también conocido como red neuronal multicapa o MLP (por sus siglas en inglés, Multilayer Perceptron), es un tipo de red neuronal artificial que consta de múltiples capas de neuronas interconectadas, diseñadas para abordar problemas de aprendizaje automático más complejos y no lineales.\n",
    "\n",
    "## Partes de un perceptrón\n",
    "\n",
    "1. **Neuronas o nodos:** Son las unidades fundamentales de procesamiento. Cada neurona realiza dos operaciones principales: una suma ponderada de las entradas y la aplicación de una función de activación.\n",
    "\n",
    "2. **Capas:** Un MLP consta de al menos tres capas: una capa de entrada, una o más capas ocultas y una capa de salida. La capa de entrada recibe los datos de entrada, la capa de salida produce los resultados finales y las capas ocultas realizan el procesamiento intermedio. Cuantas más capas ocultas tenga la red, más profunda será.\n",
    "\n",
    "3. **Conexiones:** Cada neurona en una capa está conectada las neuronas en la capa siguiente a través de conexiones ponderadas. Estas ponderaciones determinan la fuerza y la dirección de la influencia que una neurona tiene sobre las neuronas de la capa siguiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YakQCqvkRtTI"
   },
   "source": [
    "## Tipos de capas\n",
    "\n",
    "Un Perceptrón Multicapa (MLP) consta de varias capas, cada una con un propósito específico en el procesamiento de datos:\n",
    "\n",
    "1. **Capa de Entrada:**\n",
    "   - Esta es la primera capa de la red y recibe los datos de entrada. Cada nodo en esta capa representa una característica o una variable de entrada.\n",
    "   - No realiza ningún cálculo, simplemente pasa las entradas a las capas ocultas.\n",
    "\n",
    "2. **Capas Ocultas:**\n",
    "   - Las capas ocultas se encuentran entre la capa de entrada y la capa de salida y son responsables de procesar y transformar los datos de entrada\n",
    "   - Estas capas son las que permiten al MLP modelar relaciones complejas y extraer **características** relevantes de los datos.\n",
    "   - El número y el tamaño de las capas ocultas pueden variar según el diseño de la red y la complejidad del problema.\n",
    "   - Usualmenete estan totalmente conectadas, es decir, cada neurona de la capa esta conectada a todas las neuronas de la capa anterior.\n",
    "\n",
    "4. **Capa de Salida:**\n",
    "   - Esta es la última capa de la red y produce los resultados finales de la red neuronal.\n",
    "   - El número de neuronas en esta capa depende de la naturaleza del problema. Por ejemplo una capa de salida de tamaño 2 está diseñada para tareas con dos posibles salidas o clases.\n",
    "   - La función de activación en la capa de salida puede variar según el tipo de problema, como softmax para clasificación o lineal para regresión.\n",
    "\n",
    "**Discusión**\n",
    "\n",
    "¿Porque podríamos necesitar multiples neuronas en la capa de salida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZyC3woSO9oV"
   },
   "source": [
    "![perceptron multicapa](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn.analyticsvidhya.com%2Fwp-content%2Fuploads%2F2020%2F02%2FANN-Graph.gif&f=1&nofb=1&ipt=227c207c76ad1fae69299f4b776e1e16f0c2d07592a1f81ba237e961a0dddb30&ipo=images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5131nn8QAUl"
   },
   "source": [
    "## Cantidad de parámetros de un MLP\n",
    "\n",
    "Para calcular el número de parámetros en la red neuronal multicapa anterior, debemos tener en cuenta las conexiones ponderadas entre las neuronas y los sesgos (biases) en cada capa:\n",
    "\n",
    "1. **Capa de entrada (tamaño 3):**\n",
    "   - No hay parámetros en la capa de entrada en sí, ya que simplemente toma las entradas y las pasa a las neuronas de la capa oculta.\n",
    "\n",
    "2. **Capa oculta 1 (tamaño 4):**\n",
    "   - Para cada neurona en esta capa, debemos considerar sus conexiones con las neuronas de la capa de entrada. Dado que tenemos una capa de entrada de tamaño 3, cada neurona de la capa oculta 1 tendrá 3 conexiones ponderadas (pesos) con la capa de entrada, más un sesgo (bias) único para cada neurona.\n",
    "   - Entonces, para cada neurona en la capa oculta 1, se tendrá 3 pesos + 1 sesgo.\n",
    "   - Como hay 4 neuronas en esta capa, el número total de parámetros será:  \n",
    "     4 (neuronas) x (3 (pesos) + 1 (sesgo)) = 16 parámetros en la capa oculta 1.\n",
    "\n",
    "3. **Capa oculta 2 (tamaño 3):**\n",
    "   - De manera similar a la capa oculta 1, cada neurona en la capa oculta 2 tendrá conexiones con las 4 neuronas de la capa oculta 1, más un sesgo.\n",
    "   - Entonces, para cada neurona en la capa oculta 2, se tendrá 4 conexiones (pesos) + 1 sesgo.\n",
    "   - Dado que hay 3 neuronas en esta capa, el número total de parámetros será:  \n",
    "     3 (neuronas) x (4 (pesos) + 1 (sesgo)) = 15 parámetros en la capa oculta 2.\n",
    "\n",
    "4. **Capa de salida (tamaño 2):**\n",
    "   - Finalmente, en la capa de salida, cada neurona tendrá conexiones con las 3 neuronas de la capa oculta 2, más un sesgo.\n",
    "   - Entonces, para cada neurona en la capa de salida, se tendrá 3 conexiones (pesos) + 1 sesgo.\n",
    "   - Como hay 2 neuronas en esta capa, el número total de parámetros será:  \n",
    "     2 (neuronas) x (3 (pesos) + 1 (sesgo)) = 8 parámetros en la capa de salida.\n",
    "\n",
    "Ahora, se puede sumar el número total de parámetros en todas las capas del MLP:\n",
    "\n",
    "16 (capa oculta 1) + 15 (capa oculta 2) + 8 (capa de salida) = 39 parámetros en total.\n",
    "\n",
    "Por lo tanto, tu MLP tiene un total de 39 parámetros, que incluyen pesos y sesgos, y estos parámetros son los valores que se ajustarán durante el proceso de entrenamiento.\n",
    "\n",
    "\n",
    "**Ejericico**\n",
    "¿Cuántos parámetros tendrá un MLP con capas de tamaño 10, 16, 3?          \n",
    "\n",
    "16x(10+1) + 3x(16+1) = 227\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep83XRC-dHG2"
   },
   "source": [
    "## Propagación hacia adelante\n",
    "\n",
    "También conocida como \"forward propagation\" en inglés, es el proceso mediante el cual una red neuronal toma datos de entrada y los pasa a través de sus capas de neuronas para calcular una salida o predicción.\n",
    "\n",
    "1. **Entrada:** Los datos de entrada se proporcionan a la capa de entrada de la red neuronal. Cada característica o entrada se asocia con una neurona en esta capa.\n",
    "\n",
    "2. **Cálculo en Capas Ocultas:** Los datos de entrada se propagan desde la capa de entrada a través de todas las capas ocultas de la red. En cada capa oculta, cada neurona calcula una suma ponderada de las entradas y aplica una función de activación a esa suma. La salida de cada neurona en una capa se convierte en la entrada de la siguiente capa.\n",
    "\n",
    "3. **Capa de Salida:** Finalmente, la información se propaga a través de la última capa de la red, que es la capa de salida. Las neuronas en esta capa producen la salida final de la red, que puede ser una clasificación, una predicción numérica o cualquier otro tipo de resultado dependiendo de la tarea que la red esté diseñada para realizar.\n",
    "\n",
    "Se llama así porque los datos fluyen desde la entrada hacia la salida a través de la red, calculando progresivamente representaciones más abstractas y características útiles a medida que avanzan en las capas ocultas. Cada neurona en cada capa oculta contribuye a la construcción de características más complejas y no lineales en los datos, lo que permite a la red aprender y modelar relaciones complicadas en los datos de entrada.\n",
    "\n",
    "\n",
    "**Discusión**\n",
    "\n",
    "¿Que capturan las capas de un MLP?\n",
    "\n",
    "https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASGnjp2Wdyij"
   },
   "source": [
    "## Implementación Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1694974461391,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "IzRIN59yWazD"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1694976096048,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "t4E5FrIPMGJk"
   },
   "outputs": [],
   "source": [
    "class Neurona():\n",
    "\n",
    "    def __init__(self, tamanio_entrada, funcion_activacion):\n",
    "        self.pesos = np.random.rand(tamanio_entrada)\n",
    "        self.bias = np.random.rand()\n",
    "        self.funcion_activacion = funcion_activacion\n",
    "\n",
    "    def valor_neto(self,entrada):\n",
    "        return np.matmul(entrada, self.pesos) + self.bias\n",
    "\n",
    "    def salida(self,entrada):\n",
    "        print(f\"\\t\\tEntrada: {entrada}\")\n",
    "        salida = self.funcion_activacion(self.valor_neto(entrada))\n",
    "        print(f\"\\t\\tSalida: {salida}\")\n",
    "        return salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1694976100373,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "cjcfy6CfYG5C",
    "outputId": "f345f8fa-da46-46e8-c01b-bdd9ee3d9a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagacion red\n",
      "Entrada: [0.5 0.7]\n",
      "\tCapa 0 de 3\n",
      "\t\tNeurona 0 de 4\n",
      "\t\tEntrada: [0.5 0.7]\n",
      "\t\tSalida: 1.5590848695171284\n",
      "\t\tNeurona 1 de 4\n",
      "\t\tEntrada: [0.5 0.7]\n",
      "\t\tSalida: 0.7022532701982452\n",
      "\t\tNeurona 2 de 4\n",
      "\t\tEntrada: [0.5 0.7]\n",
      "\t\tSalida: 1.042814715906348\n",
      "\t\tNeurona 3 de 4\n",
      "\t\tEntrada: [0.5 0.7]\n",
      "\t\tSalida: 1.1867736936764461\n",
      "\tCapa 1 de 3\n",
      "\t\tNeurona 0 de 3\n",
      "\t\tEntrada: [1.55908487 0.70225327 1.04281472 1.18677369]\n",
      "\t\tSalida: 2.7842511429362826\n",
      "\t\tNeurona 1 de 3\n",
      "\t\tEntrada: [1.55908487 0.70225327 1.04281472 1.18677369]\n",
      "\t\tSalida: 2.0283120117426936\n",
      "\t\tNeurona 2 de 3\n",
      "\t\tEntrada: [1.55908487 0.70225327 1.04281472 1.18677369]\n",
      "\t\tSalida: 2.6907652913707554\n",
      "\tCapa 2 de 3\n",
      "\t\tNeurona 0 de 2\n",
      "\t\tEntrada: [2.78425114 2.02831201 2.69076529]\n",
      "\t\tSalida: 0.9675537310495217\n",
      "\t\tNeurona 1 de 2\n",
      "\t\tEntrada: [2.78425114 2.02831201 2.69076529]\n",
      "\t\tSalida: 0.99863162173452\n",
      "Salida: [0.96755373 0.99863162]\n"
     ]
    }
   ],
   "source": [
    "class Capa:\n",
    "    def __init__(self, tamanio_entrada, numero_neuronas, funcion_activacion):\n",
    "        self.neuronas = []\n",
    "        for _ in range(numero_neuronas):\n",
    "            neurona = Neurona(tamanio_entrada, funcion_activacion)\n",
    "            self.neuronas.append(neurona)\n",
    "\n",
    "    def propaga_adelante(self, entrada):\n",
    "        salida = []\n",
    "        for i, neurona in enumerate(self.neuronas):\n",
    "            print(f\"\\t\\tNeurona {i} de {len(self.neuronas)}\")\n",
    "            salida_neurona = neurona.salida(entrada)\n",
    "            salida.append(salida_neurona)\n",
    "        return np.array(salida)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, tamanio_entrada):\n",
    "        self.tamanio_entrada = tamanio_entrada\n",
    "        self.tamanio_salida = tamanio_entrada\n",
    "        self.capas_ocultas = []\n",
    "\n",
    "    def agrega_capa(self, numero_neuronas, funcion_activacion):\n",
    "        capa = Capa(self.tamanio_salida, numero_neuronas, funcion_activacion)\n",
    "        self.capas_ocultas.append(capa)\n",
    "        self.tamanio_salida = numero_neuronas\n",
    "\n",
    "    def propaga_adelante(self, entrada):\n",
    "        print(\"Propagacion red\")\n",
    "        print(f\"Entrada: {entrada}\")\n",
    "        salida = entrada\n",
    "        for i,capa in enumerate(self.capas_ocultas):\n",
    "            print(f\"\\tCapa {i} de {len(self.capas_ocultas)}\")\n",
    "            salida = capa.propaga_adelante(salida)\n",
    "        print(f\"Salida: {salida}\")\n",
    "        return salida\n",
    "\n",
    "\n",
    "def funcion_activacion_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def funcion_activacion_lineal(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "entrada = np.array([0.5, 0.7])\n",
    "\n",
    "mlp = MLP(tamanio_entrada=len(entrada))\n",
    "\n",
    "mlp.agrega_capa(numero_neuronas=4, funcion_activacion=funcion_activacion_lineal)\n",
    "mlp.agrega_capa(numero_neuronas=3, funcion_activacion=funcion_activacion_lineal)\n",
    "mlp.agrega_capa(numero_neuronas=2, funcion_activacion=funcion_activacion_sigmoid)\n",
    "\n",
    "\n",
    "resultado = mlp.propaga_adelante(entrada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchas maneras de bajar las salidas (0.96755373 0.99863162) solo es basura, no esta entrenada, lo esperado es que los números no sean tan iguales (pueden sumar más de uno, porque no se refieren a probabilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDdspNgse5PM"
   },
   "source": [
    "## Implementación Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3256,
     "status": "ok",
     "timestamp": 1694976838086,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "gA51lwcafKoS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 12:07:00.136482: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-23 12:07:00.530713: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-23 12:07:00.532508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-23 12:07:01.971809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1694976838257,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "iogoeOpfcQv3",
    "outputId": "bafcf1d5-daf2-4e0f-e286-4313defd743f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 16        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 15        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39 (156.00 Byte)\n",
      "Trainable params: 39 (156.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Implementación de gif de inicio de notebook\n",
    "modelo = Sequential()\n",
    "\n",
    "modelo.add(Dense(4, input_dim=3, activation='relu'))\n",
    "modelo.add(Dense(3, activation='relu'))\n",
    "modelo.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_eH-Aclfnsm"
   },
   "source": [
    "**Discusión**:\n",
    "\n",
    "¿Qué son parametros no entrenables?        \n",
    "En situaciones que no queremos entrenar toda la red entrena todas las caracteristicas bien pero la capa de salida no esta bien, entonces con los parametros no entrenables, se dejan intactos pesos y bias de ciertas capas, y se sentra en cierta sección \n",
    "\n",
    "\n",
    "Redes generativas adversariales THA FUCK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbbRwEveh1ez"
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Recordemos que una manera de entrar una neurona es a traves de descenso de gradiente sobre la función de périda.\n",
    "\n",
    "Cuando tenenemos varias capas, tenemos que realizar un proceso llamado Propagación hacia atras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVSgztQ9gC3c"
   },
   "source": [
    "\n",
    "\n",
    "### Propagación hacia atras\n",
    "\n",
    "\n",
    "También conocida como \"backpropagation\" en inglés, es el algoritmo fundamental utilizado en el entrenamiento de redes neuronales. Permite ajustar los pesos y sesgos de las neuronas en la red para que pueda aprender a hacer predicciones precisas.\n",
    "\n",
    "1. **Inicialización de pesos y sesgos:**\n",
    "   - Antes de comenzar el proceso de entrenamiento, los pesos y sesgos de todas las neuronas en la red se inicializan típicamente con valores aleatorios ***pequeños.***\n",
    "\n",
    "2. **Paso de Propagación Hacia Adelante:**\n",
    "   - Se toma un conjunto de datos de entrenamiento y se pasa a través de la red neuronal desde la capa de entrada hasta la capa de salida utilizando la propagación hacia adelante, como se explicó anteriormente.\n",
    "   - Las salidas calculadas por la red se comparan con las salidas reales conocidas para calcular el error, que mide qué tan lejos están las predicciones de las respuestas correctas.\n",
    "\n",
    "3. **Cálculo del Gradiente Descendente en la Capa de Salida:**\n",
    "   - El objetivo del entrenamiento es minimizar el error entre las predicciones y las respuestas correctas. Para hacer esto, primero calculamos la derivada parcial del error con respecto a los pesos de la red.\n",
    "   - Esto se hace utilizando el gradiente descendente y se basa en la derivada de la función de pérdida.\n",
    "\n",
    "4. **Propagación Hacia Atrás de los Errores:**\n",
    "   - Una vez que tenemos el gradiente en la capa de salida, lo propagamos hacia atrás a través de la red para calcular los gradientes en las capas ocultas.\n",
    "   - Comenzamos calculando las derivadas parciales del error con respecto a las salidas de las neuronas en la última capa oculta.\n",
    "\n",
    "5. **Actualización de Pesos y Sesgos:**\n",
    "   - Utilizamos los gradientes calculados para ajustar los pesos y sesgos en la red neuronal. El objetivo es modificar los parámetros de manera que el error se reduzca.\n",
    "   - Este proceso se repite para cada conjunto de entrenamiento en el conjunto de datos.\n",
    "\n",
    "6. **Iteración y Convergencia:**\n",
    "   - Los pasos anteriores se repiten para múltiples iteraciones (épocas) a través de todo el conjunto de datos de entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uqZ3864kRN_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZdG3frklKPR"
   },
   "source": [
    "Si se toma de ejemplo una red de\n",
    "- una entrada,\n",
    "- una capa oculta con una sola neurona\n",
    "- una salida\n",
    "\n",
    "Si se quiere entrenar esta red mediante descenso de gradiente se debe calcular la derivada de la\n",
    "\n",
    "Fórmula de Pérdida (Error Cuadrático Medio - MSE):\n",
    "\n",
    "$$L = \\frac{1}{2}(y_{\\text{real}} - y_{\\text{pred}})^2$$\n",
    "\n",
    "Donde:\n",
    "- $L$ es el valor de la función de pérdida (MSE).\n",
    "- $y_{\\text{real}}$ es el valor real o esperado de salida.\n",
    "- $y_{\\text{pred}}$ es el valor predicho por la red neuronal.\n",
    "\n",
    "Si claculamos que es $y_{\\text{pred}}$\n",
    "\n",
    "$$L = \\frac{1}{2}(y_{\\text{real}} - f^{(2)}( w_1^{(2)} a_1 + \\theta ^{(2)} ) )^2$$\n",
    "\n",
    "Donde:\n",
    "- $f^{(2)}$ es la funcion deactivación de la capa de salida\n",
    "- $w_1^{(2)}$ es el peso asociado a la capa de salida\n",
    "- $\\theta ^{(2)}$ es el sesgo asociado a la capa de salida\n",
    "- $a_1$ es la salida de la capa oculta\n",
    "\n",
    "Yendo una capa mas profundo\n",
    "\n",
    "$$L = \\frac{1}{2}(y_{\\text{real}} - f^{(2)}( w_1^{(2)} f^{(1)}(w_1^{(1)} x_1 + \\theta ^{(1)} ) + \\theta ^{(2)} ) )^2$$\n",
    "\n",
    "Donde:\n",
    "- $f^{(1)}$ es la funcion deactivación de la capa oculta\n",
    "- $w_1^{(1)}$ es el peso asociado a la capa oculta\n",
    "- $\\theta ^{(1)}$ es el sesgo asociado a la oculta\n",
    "- $x_1$ es el valor de la capa de entrada\n",
    "\n",
    "\n",
    "Esta es la función que debemos derivar\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{w_1^{(2)}} = (y_{\\text{real}} - y_{\\text{pred}}) (-f'^{(2)}( w_1^{(2)} a_1 + \\theta ^{(2)} )a_1) $$\n",
    "$$\\frac{\\partial L}{\\theta ^{(2)}} = (y_{\\text{real}} - y_{\\text{pred}}) (-f'^{(2)}( w_1^{(2)} a_1 + \\theta ^{(2)} )) $$\n",
    "\n",
    "$$\\frac{\\partial L}{w_1^{(1)}} = (y_{\\text{real}} - y_{\\text{pred}}) (-f'^{(2)}( w_1^{(2)} a_1 + \\theta ^{(2)} ) ( w_1^{(2)}  f'^{(1)}(w_1^{(1)} x_1 + \\theta ^{(1)} )) x_1 ) $$\n",
    "$$\\frac{\\partial L}{\\theta ^{(1)}} = (y_{\\text{real}} - y_{\\text{pred}}) (-f'^{(2)}( w_1^{(2)} a_1 + \\theta ^{(2)} ) ( w_1^{(2)}  f'^{(1)}(w_1^{(1)} x_1 + \\theta ^{(1)} )) ) $$\n",
    "\n",
    "\n",
    "**Discusión**\n",
    "\n",
    "¿Que sucede si agregamos mas neuronas/capas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkmc79MutoBo"
   },
   "source": [
    "### Implementación Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1694980662362,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "GNFT-9h3thB3",
    "outputId": "68fe5baa-3107-4d1d-e3d5-a79e6092050f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 4)                 16        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 15        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39 (156.00 Byte)\n",
      "Trainable params: 39 (156.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo = Sequential()\n",
    "\n",
    "modelo.add(Dense(4, input_dim=3, activation='relu'))\n",
    "modelo.add(Dense(3, activation='relu'))\n",
    "modelo.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "modelo.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcYbVCjWuGJg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOjCpU6mNwlVeFREsr5BUY6",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
