{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J8RUOAJVsdk"
   },
   "source": [
    "# Redes Generativas Adversariales (GAN)\n",
    "\n",
    "Las **Redes Generativas Adversariales (GAN)** son un tipo de modelo de aprendizaje profundo que consiste en dos redes neuronales, un generador y un discriminador, que son entrenadas simultáneamente a través de un proceso adversarial. Fueron propuestas por Ian Goodfellow y sus colegas en 2014.\n",
    "\n",
    "## Estructura Básica\n",
    "\n",
    "1. **Generador:** Crea datos sintéticos, como imágenes, a partir de un conjunto de datos de entrada. El objetivo es que las muestras generadas por el generador sean indistinguibles de las muestras reales.\n",
    "\n",
    "2. **Discriminador:** Evalúa si una muestra es real (proveniente del conjunto de datos de entrenamiento) o falsa (generada por el generador). Su objetivo es mejorar su capacidad para distinguir entre muestras reales y generadas.\n",
    "\n",
    "## Proceso de Entrenamiento\n",
    "\n",
    "El entrenamiento de una GAN implica un juego de adversarios entre el generador y el discriminador:\n",
    "\n",
    "1. **Fase de Generación:**\n",
    "   - El generador crea muestras sintéticas.\n",
    "\n",
    "2. **Fase de Discriminación:**\n",
    "   - El discriminador también evalúa muestras reales y emite una probabilidad de autenticidad.\n",
    "   - Se calcula la pérdida (diferencia entre las predicciones y las etiquetas reales) para ambas muestras reales y generadas.\n",
    "\n",
    "3. **Optimización Conjunta:**\n",
    "   - Se optimiza el generador para engañar al discriminador, minimizando la pérdida adversarial.\n",
    "   - Se optimiza el discriminador para mejorar su capacidad de distinguir entre real y sintético, minimizando su pérdida.\n",
    "\n",
    "Este proceso se repite iterativamente hasta que el generador produce muestras casi indistinguibles de las reales y el discriminador no puede diferenciar entre las dos con certeza.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceTY-krIZrIt"
   },
   "source": [
    "## Estructura\n",
    "\n",
    "La estructura de una Red Generativa Adversarial (GAN) se compone de dos redes principales: el generador y el discriminador. Ambas redes están entrenadas de manera adversarial, es decir, una red intenta mejorar continuamente mientras la otra intenta detectar las muestras generadas.\n",
    "\n",
    "### Generador\n",
    "\n",
    "El generador tiene la tarea de crear datos sintéticos que sean indistinguibles de los datos reales. Tiene una arquitectura de red que transforma un vector de entrada (a menudo llamado de un espacio latente) en una muestra artificial. Este vector de entrada puede ser un vector aleatorio (ruido) para poder producir una serie elmentos distintos.\n",
    "\n",
    "### Discriminador\n",
    "\n",
    "El discriminador evalúa si una muestra dada es real o generada por el generador. Tiene la tarea de mejorar su capacidad para hacer esta distinción a medida que avanza el entrenamiento.\n",
    "\n",
    "\n",
    "![](https://www.jisc.ac.uk/sites/default/files/gans-diagram.jpg)\n",
    "\n",
    "## Proceso de Entrenamiento\n",
    "\n",
    "1. **Fase de Generación:**\n",
    "   - El generador toma muestras del espacio latente y produce datos sintéticos.\n",
    "   - Estos datos sintéticos se envían al discriminador.\n",
    "\n",
    "2. **Fase de Discriminación:**\n",
    "   - El discriminador evalúa las muestras reales y generadas, emitiendo probabilidades de autenticidad.\n",
    "   - Se calcula la pérdida adversarial basada en cuán bien el discriminador puede distinguir entre las muestras reales y generadas.\n",
    "\n",
    "3. **Optimización Conjunta:**\n",
    "   - Se optimiza el generador para engañar al discriminador, minimizando la pérdida adversarial.\n",
    "   - Se optimiza el discriminador para mejorar su capacidad de distinguir entre datos reales y generados, minimizando su pérdida.\n",
    "\n",
    "Este proceso se repite iterativamente hasta que el nivel de calidad de elementos creados por el generador sea aceptab\n",
    "\n",
    "### Retroalimentación Adversarial\n",
    "\n",
    "La clave de una GAN es la competencia entre el generador y el discriminador. A medida que el generador mejora, el discriminador también debe mejorar para mantener la distinción entre datos reales y generados. Esta retroalimentación adversarial es fundamental para el éxito de la GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uz0YdrEfIaa"
   },
   "source": [
    "\n",
    "# Distintas arquitecturas de GANs\n",
    "\n",
    "A lo largo del tiempo, varias arquitecturas de GANs han sido propuestas para abordar diferentes problemas y mejorar la estabilidad del entrenamiento. Algunas de las arquitecturas más conocidas son:\n",
    "\n",
    "1. **DCGAN (Deep Convolutional GAN):**\n",
    "   - Introduce capas de convolución en el generador y el discriminador, mejorando la capacidad de trabajar con datos de imagen. DCGAN ha sido ampliamente utilizado para la generación de imágenes realistas.\n",
    "\n",
    "2. **WGAN (Wasserstein GAN):**\n",
    "   - Propone una modificación en la función de pérdida para mejorar la estabilidad del entrenamiento, utilizando la distancia de Wasserstein. WGAN aborda problemas como el colapso del modo y proporciona una métrica más significativa de la calidad de las muestras generadas.\n",
    "\n",
    "3. **CGAN (Conditional GAN):**\n",
    "   - Extiende la GAN básica permitiendo la generación condicional, es decir, especificar una clase o características específicas que se desean en la muestra generada. Es útil para tareas de generación condicional donde se quiere controlar la salida.\n",
    "\n",
    "4. **CycleGAN:**\n",
    "   - Diseñado para problemas de traducción de estilo, como la conversión de imágenes de un dominio a otro sin necesidad de pares de entrenamiento específicos. CycleGAN utiliza la consistencia cíclica para mejorar la calidad de la traducción.\n",
    "\n",
    "5. **PGGAN (Progressive GAN):**\n",
    "   - Propone una arquitectura progresiva que comienza con imágenes de baja resolución y las va incrementando gradualmente. PGGAN ha demostrado ser eficaz para generar imágenes de alta resolución.\n",
    "\n",
    "6. **StyleGAN y StyleGAN2:**\n",
    "   - Introducen conceptos de estilo y control más fino en la generación de imágenes. StyleGAN permite la manipulación del estilo de las muestras generadas, mientras que StyleGAN2 mejora la calidad visual y la estabilidad del entrenamiento.\n",
    "\n",
    "7. **BigGAN:**\n",
    "   - Diseñado para manejar grandes conjuntos de datos y escalarse eficientemente. BigGAN ha demostrado ser eficaz en la generación de imágenes de alta calidad y resolución.\n",
    "\n",
    "8. **SN-GAN (Spectral Normalization GAN):**\n",
    "   - Utiliza la normalización espectral para estabilizar el entrenamiento, mejorando la capacidad del discriminador para manejar múltiples modos en el espacio de datos.\n",
    "\n",
    "9. **SAGAN (Self-Attention GAN):**\n",
    "   - Incorpora mecanismos de autoatención para permitir que la red se enfoque en diferentes regiones de la imagen durante la generación, mejorando la calidad de las muestras.\n",
    "\n",
    "Estas son solo algunas de las arquitecturas más conocidas de GANs, y la investigación en este campo sigue evolucionando, dando lugar a nuevas propuestas y mejoras constantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jfJN60fWVHf"
   },
   "source": [
    "# Problemáticas con GANs\n",
    "\n",
    "\n",
    "Aunque las Redes Generativas Adversariales (GAN) han demostrado ser poderosas en diversas aplicaciones, también presentan desafíos y problemáticas que deben abordarse:\n",
    "\n",
    "- **Inestabilidad en el Entrenamiento:** El entrenamiento de GANs puede ser inestable y desafiante. A veces, el discriminador se vuelve demasiado dominante, afectando la convergencia.\n",
    "\n",
    "- **Modos Colapsados:** Las GANs pueden sufrir de modos colapsados, donde el generador solo produce un conjunto limitado de muestras, ignorando la diversidad en los datos de entrenamiento.\n",
    "\n",
    "- **Evaluación de Calidad:** Evaluar la calidad de las muestras generadas por GANs no es trivial. Las métricas objetivas pueden no capturar completamente la percepción humana de la calidad visual, lo que hace que la evaluación sea subjetiva.\n",
    "\n",
    "- **Sensibilidad a Hiperparámetros:** GANs son sensibles a la selección de hiperparámetros, como tasas de aprendizaje, arquitecturas de red y dimensiones del espacio latente. Encontrar la combinación óptima puede requerir ajuste y experimentación.\n",
    "\n",
    "- **Generación de Datos No Deseados:** En algunos casos, los modelos GAN pueden generar datos que son biológicamente poco realistas o no deseados.\n",
    "\n",
    "- **Interpretabilidad y Control:** Entender y controlar lo que aprenden las GANs puede ser desafiante. La falta de interpretabilidad puede ser un obstáculo en aplicaciones críticas, donde se necesita explicar el razonamiento detrás de las decisiones del modelo.\n",
    "\n",
    "- **Requerimientos Computacionales:** El entrenamiento de GANs a menudo requiere grandes cantidades de recursos computacionales, tiempo y potencia de cálculo, especialmente para arquitecturas más grandes y conjuntos de datos masivos.\n",
    "\n",
    "- **Ética y Uso Malintencionado:** La capacidad de GANs para generar contenido falso y realista plantea preocupaciones éticas, como la creación de deepfakes, que pueden ser utilizados con intenciones maliciosas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1700889812616,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "Hsp2j8B93-pe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 10:13:35.200673: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-25 10:13:35.941597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-25 10:13:35.941654: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-25 10:13:35.945947: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-25 10:13:36.323916: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-25 10:13:36.327599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-25 10:13:39.864862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1700888817365,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "COw6KuVx30Bp"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1  #monocromática por ello solo 1\n",
    "num_classes = 10  # 10 digitos\n",
    "image_size = 28\n",
    "latent_dim = 128  # ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2282,
     "status": "ok",
     "timestamp": 1700888850679,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "QyXLdr-K3z-Y",
    "outputId": "6b39281f-9e0a-4ef5-b4b0-34fbf10d5aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (70000, 28, 28, 1)\n",
      "Shape of training labels: (70000, 10)\n"
     ]
    }
   ],
   "source": [
    "# We'll use all the available examples from both the training and test\n",
    "# sets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_labels = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# the images, and one-hot encode the labels.\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "all_labels = keras.utils.to_categorical(all_labels, 10)\n",
    "\n",
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))   # ayuda a consumir más eficientemente\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)        #mezclar para que haya de todos los números\n",
    "\n",
    "print(f\"Shape of training images: {all_digits.shape}\")\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1700888869393,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "aVWX2wIq3z7e",
    "outputId": "905ec0f0-71e3-41fc-dc6d-5da8e42bfba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "# CONDITIONAL GAN vamos a poder decidir que número generar\n",
    "generator_in_channels = latent_dim + num_classes   # ruido más clase\n",
    "discriminator_in_channels = num_channels + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1700888884247,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "OlzzewIX3z4b"
   },
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(7 * 7 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((7, 7, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),  #transpuestas == capa de deconvolución\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1700888898688,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "93ONvdpC3z1f"
   },
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")  #seguimiento de perdido de generador\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")   #\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)    #analizar vs reshape de arriba\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1     #unimos ruido con imagenes reales\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "\n",
    "        #uno es falso cero es real\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        #aplicar gradientes una vez a los pesos que se pueden actualizar\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        ### Concluye entrenamiento de discriminador\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        ## vamos a decir que todos son reales  (que te faltó para que el discriminador te hubiera creído todo)\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 651394,
     "status": "ok",
     "timestamp": 1700889561436,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "G3JPv73f3zyW",
    "outputId": "1cc887e0-b2a6-4b82-aa02-493c5fcc9ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1094/1094 [==============================] - 1205s 1s/step - g_loss: 1.4603 - d_loss: 0.4140\n",
      "Epoch 2/20\n",
      "1094/1094 [==============================] - 1233s 1s/step - g_loss: 1.0761 - d_loss: 0.5626\n",
      "Epoch 3/20\n",
      "1094/1094 [==============================] - 1233s 1s/step - g_loss: 1.4681 - d_loss: 0.4103\n",
      "Epoch 4/20\n",
      "1094/1094 [==============================] - 1174s 1s/step - g_loss: 1.8460 - d_loss: 0.3231\n",
      "Epoch 5/20\n",
      " 225/1094 [=====>........................] - ETA: 15:04 - g_loss: 0.9728 - d_loss: 0.6054"
     ]
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),   #o es real o es falso\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1360,
     "status": "ok",
     "timestamp": 1700889562781,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "hgd3qPce3zve",
    "outputId": "0443bec2-7f73-488a-f47a-5058e023ff63"
   },
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditional GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 9  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n",
    "\n",
    "start_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "end_class = 5  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "\n",
    "fake_images = interpolate_class(start_class, end_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1700889853903,
     "user": {
      "displayName": "Irving Gibrán Cabrera Zamora",
      "userId": "00600026942839119351"
     },
     "user_tz": 360
    },
    "id": "TEJZKmNU3zsO",
    "outputId": "8443d3b1-657f-41d7-b9e1-4e567d2069fa"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(fake_images.shape[0]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(fake_images[i], interpolation='nearest', cmap='gray_r')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMO0W6CyivhEyge2K6mMRoG",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
