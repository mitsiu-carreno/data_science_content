{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AnJB8GhB9rD"
   },
   "source": [
    "# Examen práctico del primer parcial\n",
    "\n",
    "Utiliza solo librerías de álgebra matricial como numpy y scipy. Librerías de visualización y cualquier función que sea parte del core de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSi5PcRSCF4d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4fKiGVHB9n4"
   },
   "source": [
    "## 1 Implementa una neurona con sus componentes básicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StrarMBNCQTb"
   },
   "outputs": [],
   "source": [
    "class Neurona00():\n",
    "    def __init__(self, tamanio_entrada):\n",
    "        self.pesos = np.random.rand(tamanio_entrada)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "class Neurona01(Neurona00):\n",
    "    def valor_neto(self,entrada):\n",
    "        return np.matmul(entrada, self.pesos) + self.bias\n",
    "\n",
    "class Neurona02(Neurona01):\n",
    "\n",
    "    def __init__(self, tamanio_entrada, funcion_activacion):\n",
    "        self.pesos = np.random.rand(tamanio_entrada)\n",
    "        self.bias = np.random.rand()\n",
    "        self.funcion_activacion = funcion_activacion\n",
    "\n",
    "    def salida(self,entrada):\n",
    "        return self.funcion_activacion(self.valor_neto(entrada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-eQK92pBIpk"
   },
   "source": [
    "\n",
    "## 1.1 Enumera cuales son estos componentes\n",
    "\n",
    "### 1.1.1 Las entradas\n",
    "Es la cantidad de señales que va a recibir la neurona\n",
    "\n",
    "Depende de la dimensionalidad de nuestro problema, en este caso son imágenes de 8x8 por lo que tenemos 64 entradas.\n",
    "\n",
    "### 1.1.2 Los pesos\n",
    "Debemos de tener un peso para cada una de las entradas, lo que si podemos elegir es como se inicializan estos pesos. Algunas opciones son\n",
    "\n",
    "1. Aleatorios\n",
    "2. iniciales en 0\n",
    "3. Determinados de manera fija por conocimiento del problema\n",
    "\n",
    "Se elige inicializarlo de manera aleatoria ya que no se cuenta con información previa.\n",
    "\n",
    "### 1.1.3. El sesgo o bias\n",
    "Es un peso que no esta asociado a ninguna entrada, al igual que los pesos tiene varias maneras de inicializase.\n",
    "\n",
    "### 1.1.4. Valor neto\n",
    "Es la función que integra todas las entradas y las condensa en un solo numero. Aunque existen otras implementaciones la mas común es la lineal la cual consiste en multiplicar cada entrada por su peso asociado y sumar el bias.\n",
    "\n",
    "### 1.1.5 La Función de Activación\n",
    "Es la fución encargada de definir si la neurona se activa o no.\n",
    "\n",
    "Para esto hay una gran selección de funciones\n",
    "\n",
    "- Sigmoide: Utiliza la función sigmoide, que produce una salida en el rango (0, 1).\n",
    "- ReLU: Emplea la función ReLU, que produce una salida lineal si es positiva y cero si es negativa.\n",
    "- Tangente hiperbólica: Utiliza la función tanh, que produce una salida en el rango (-1, 1).\n",
    "- Función lineal: Utiliza una función lineal que en realidad pasa el valor como es\n",
    "Como este problema es de clasificación la selección mas segura es la función sigmoide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkf0v4UMGGUt"
   },
   "source": [
    "# 2 Agrega a la implementación necesaria para entrenar la neurona         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apkNWxZ5GJ-F"
   },
   "outputs": [],
   "source": [
    "class Neurona03(Neurona02):\n",
    "\n",
    "    def compila(self,funcion_perdida,derivada_funcion_perdida_pesos,derivada_funcion_perdida_bias):\n",
    "        self.funcion_perdida = funcion_perdida\n",
    "        self.derivada_funcion_perdida_pesos = derivada_funcion_perdida_pesos\n",
    "        self.derivada_funcion_perdida_bias = derivada_funcion_perdida_bias\n",
    "\n",
    "\n",
    "    def calcula_error(self,entradas,y_real):\n",
    "        return self.funcion_perdida([self.salida(entradas)] ,y_real)\n",
    "\n",
    "    def decenso_gradiente_step(self, X, y):\n",
    "        # todas las variables son vectores (n muestras) excepto total_error\n",
    "        valor_neto = self.valor_neto(X)\n",
    "        y_pred = self.funcion_activacion(valor_neto)\n",
    "        total_error = self.funcion_perdida(y,y_pred)\n",
    "        gradiente_pesos = self.derivada_funcion_perdida_pesos(X,y,y_pred)\n",
    "        gradiente_bias = self.derivada_funcion_perdida_bias(X,y,y_pred)\n",
    "\n",
    "        return gradiente_pesos, gradiente_bias, total_error\n",
    "\n",
    "    def desenso_gradiente(self,epochs, X, y, tasa_aprendizaje):\n",
    "        lista_error = []\n",
    "        for epoch in range(epochs):\n",
    "            gradiente_pesos, gradiente_bias, total_error = self.decenso_gradiente_step( X, y)\n",
    "            self.pesos = self.pesos - (tasa_aprendizaje * gradiente_pesos)\n",
    "            self.bias = self.bias - (tasa_aprendizaje * gradiente_bias)\n",
    "            lista_error.append(total_error)\n",
    "            if (epoch+1) % int(epochs/20) == 0:\n",
    "                print(f\"Epoch {epoch+1}, Error: {total_error.mean()}, {self.pesos[:5]} {self.bias}\")\n",
    "        return lista_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Byl0Le4Bllx"
   },
   "source": [
    "## 2.1 De la misma manera que en el punto anterior, enumera cuales son estos componentes, para cada uno\n",
    "\n",
    "### Función de Pérdida (Loss Function):\n",
    "\n",
    "Una función que mide la discrepancia entre la salida real de la neurona y la salida esperada.\n",
    "\n",
    "**Regresión:**\n",
    "\n",
    "- [Error Cuadrático Medio](https://keras.io/api/losses/regression_losses/#meansquarederror-class) (MSE): Calcula el promedio de los cuadrados de las diferencias entre las predicciones y los valores reales.\n",
    "- [Error Absoluto Medio](https://keras.io/api/losses/regression_losses/#meanabsoluteerror-class) (MAE): Calcula el promedio de las diferencias absolutas entre las predicciones y los valores reales.\n",
    "- [Error Huber](https://keras.io/api/losses/regression_losses/#huber-class): Combina las propiedades de MSE y MAE.\n",
    "\n",
    "**Clasificación**\n",
    "\n",
    "- [Entropía Cruzada Binaria](https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class) (Binary Cross-Entropy): Es utilizada para medir la discrepancia entre las probabilidades predichas y las etiquetas verdaderas.\n",
    "- [Entropía Cruzada Categórica](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class) (Categorical Cross-Entropy): Extensión de la entropía cruzada para clasificación multiclase.\n",
    "- [Hinge Loss](https://keras.io/api/losses/hinge_losses/#hinge-class) : Utilizada en máquinas de soporte vectorial (SVM) para problemas de clasificación binaria. Es especialmente útil cuando se desea maximizar el margen entre las clases.\n",
    "\n",
    "\n",
    "Al ser un problema de clasificación binaria lo mas adecuado y simple es implementar Binary Cross-Entropy\n",
    "\n",
    "\n",
    "\n",
    "### Optimizador:\n",
    "\n",
    "Un algoritmo que ajusta los pesos y el sesgo de la neurona para minimizar la función de pérdida.\n",
    "\n",
    "\n",
    "- [SGD](https://keras.io/api/optimizers/sgd/): Descenso de gradiente con momento.\n",
    "- [RMSprop](https://keras.io/api/optimizers/rmsprop/): Utiliza medias móviles del gradiente para ajustar la tasa de aprendizaje.\n",
    "- [Adam](https://keras.io/api/optimizers/adam/): Descenso de gradiente estocástico que adapta la tasa de aprendizaje con los momentos estadísticos.\n",
    "- [AdamW](https://keras.io/api/optimizers/adamw/):\n",
    "- [Adadelta](https://keras.io/api/optimizers/adadelta/): Basado en Adam usa una tasa de aprendizaje para cada dimensión respecto a la frecuencia de actualización en una ventana.\n",
    "- [Adagrad](https://keras.io/api/optimizers/adagrad/): Basado en Adam usa una tasa de aprendizaje para cada dimensión respecto a la frecuencia de actualización en el entrenamiento.\n",
    "- [Adamax](https://keras.io/api/optimizers/adamax/): Tiene la capacidad de adaptar tasa de aprendizaje basado en las características de los datos. Por ejemplo, datos de habla con ruido variable.\n",
    "- [Adafactor](https://keras.io/api/optimizers/adafactor/): Solo guarda información parcial de los gradientes pasados.\n",
    "- [Nadam](https://keras.io/api/optimizers/Nadam/): Adam con momento de Nesterov.\n",
    "- [Ftrl](https://keras.io/api/optimizers/ftrl/): Adaptado para modelos espacios de características muy grandes.\n",
    "\n",
    "Por sencillez se opta por implementar descenso de gradiente por batches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcLqGjTQf4we"
   },
   "source": [
    "# 3 Utiliza tu implementación para clasificar la base de datos [digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vH_hJxPzf5jj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppN0WKT9hvZ6"
   },
   "source": [
    "## 3.1 Utiliza solo dos clases, seleccionadas de manera aleatoria.         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBZqn2LsgCFn"
   },
   "outputs": [],
   "source": [
    "tX, ty = load_digits(return_X_y=True)\n",
    "# Generar el primer número aleatorio entre 0 y 9\n",
    "num1 = np.random.randint(0, 9)  # Genera un número aleatorio entre 10 y 99\n",
    "# Generar el segundo número aleatorio diferente al primero\n",
    "num2 = num1\n",
    "while num2 == num1:\n",
    "    num2 = np.random.randint(0, 9)\n",
    "\n",
    "#filtramos los registros con las 2 clases aleatorias\n",
    "fy = ty[(ty ==num1) |(ty ==num2)]\n",
    "fX = tX[(ty ==num1) |(ty ==num2)]\n",
    "\n",
    "print(num1,num2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLKOtDk4h4yq"
   },
   "source": [
    "## 3.2 Realiza los pasos de pre-procesamiento que consideres necesarios para mejorar la calidad del clasificador.        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuJKghZQhOC4"
   },
   "outputs": [],
   "source": [
    "X_ = (fX-fX.mean())/fX.std()\n",
    "# Estandarizar restar la media y dividir entre desviación estandar \n",
    "# (datos centrados en 0 con desviación de 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYkADCxFiDxX"
   },
   "outputs": [],
   "source": [
    "mask_num1 = fy==num1\n",
    "mask_num2 = fy==num2\n",
    "y_ = fy\n",
    "y_[mask_num1] = 0\n",
    "y_[mask_num2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYsqOzVfkTgN"
   },
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73ZkhWfRiEE5"
   },
   "source": [
    "3.3 Entrena la neurona para predecir cuál es el número de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wRTe0RthpmC"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_derivada_pesos(X, y, y_pred):\n",
    "    return np.matmul(X.T,(y_pred-y))/len(X)\n",
    "\n",
    "def binary_cross_entropy_derivada_bias(X, y, y_pred):\n",
    "    return sum(y_pred-y)/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvOFCc_wks5k"
   },
   "outputs": [],
   "source": [
    "neurona = Neurona03(64,sigmoid)\n",
    "neurona.compila(\n",
    "    binary_cross_entropy,\n",
    "    derivada_funcion_perdida_pesos=binary_cross_entropy_derivada_pesos,\n",
    "    derivada_funcion_perdida_bias=binary_cross_entropy_derivada_bias\n",
    "    )\n",
    "ta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mIuTdpokvG9"
   },
   "outputs": [],
   "source": [
    "perdida = neurona.desenso_gradiente(epochs=10000, X=X_train, y=y_train, tasa_aprendizaje=ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1vEr0cEBleu"
   },
   "source": [
    "## 3.4 Explica las razones detrás de la selección de los distintos parámetros\n",
    "\n",
    "### 3.4.1 Epochs\n",
    "Se incrementaron de manera gradual hasta que la perdida se disminuye muy poco entre epochs. Antes de que se estanque por completo porque puede provocar sobre ajuste.\n",
    "\n",
    "#3.4.2 Tamaño de batch\n",
    "Se opta por un tamaño de batch completo ya que no son muchos datos para asegurar estabilidad\n",
    "\n",
    "#3.4.3 Criterio de alto\n",
    "El unico criterio de alto es el número de epochs\n",
    "\n",
    "#3.4.3 Tasa de aprendizaje\n",
    "\n",
    "Utilizamos una tasa de aprendizaje lo mas grande que no provoque aumentos en la perdida a través de los epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiCzYUZvBk9k"
   },
   "source": [
    "## 3.5 Obtén el valor de la pérdida (loss) y da una explicación de cuando y porque cambian estos valores (o porque no)\n",
    "\n",
    "Vemos que la perdida disminuye de manera correcta, muy rápido al principio y mas lento al final. Hay que tomar en cuenta que la escala es logarítmica. Por lo que podríamos empezar a decir que el aprendizaje comienza a estancarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zBqjL8gnNAe"
   },
   "outputs": [],
   "source": [
    "plt.plot(perdida) #perdida\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFGJRUFPpjHI"
   },
   "source": [
    "## 3.6 Evalúa el desempeño contra una muestra en la que nunca se haya entrenado.        \n",
    "### 3.6.1 Da los principales indicadores de calidad          \n",
    "### 3.6.2 Interpreta cuál es el significado cada valor de los indicadores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGvByOBfqhKh"
   },
   "outputs": [],
   "source": [
    "salida_test = neurona.salida(X_test)\n",
    "mask = salida_test < 0.5\n",
    "prediccion = np.zeros_like(salida_test)\n",
    "prediccion[salida_test > 0.5 ] = 1\n",
    "prediccion = prediccion.astype(int)\n",
    "prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1B5dL1zuskSm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, prediccion)\n",
    "accuracy = accuracy_score(y_test, prediccion)\n",
    "precision = precision_score(y_test, prediccion)\n",
    "recall = recall_score(y_test, prediccion)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkBpKe2BsPfG"
   },
   "source": [
    "Vemos que la matriz de confusión es perfecta. De la misma manera accuracy presision y recall.\n",
    "\n",
    "Vemos que no hubo sobreentrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCfnFk6kBlWW"
   },
   "source": [
    "# 4 Presenta una conclusión final del modelo creado. Cuales son las fortalezas y debilidades, así como sugerencias para mejorar su desempeño.\n",
    "para este set de datos el modelo resulto muy bueno. Aunque la cantidad de epochs podría ser excesiva, resulta peligroso aumentar la tasa de aprendizaje.\n",
    "\n",
    "Se puede mejorar el modelo con un monitoreo de métricas que nos permitan detener de manera anticipada el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BEOYRqys0Aa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNx6ELOARQCUkOaqhYClskG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
